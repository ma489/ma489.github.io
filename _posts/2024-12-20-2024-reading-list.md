---
title: 2024 Reading List
date: 2024-12-20T12:02:00+00:00
layout: single
permalink: /2024/12/2024-reading-list/
classes: wide
---

As 2024 draws to a close, it’s time for another retrospective list of the most interesting books I’ve read this year. 

The list is decidedly shorter than usual, a result of my focus on completing my Master’s degree — a year spent diving into academic papers more than leisure reading. (I've not included the papers I read for my thesis here).

Interestingly, not a single physical book made the cut this year — a sign of the times, perhaps? Additionally, most of the titles are _recent_ publications, with the obvious exception of the timeless wisdom found in Marcus Aurelius’ writings (!).

Finally, a delightful addition to my reading routine this year was to attend the excellent [_Reading Rhythms_](https://readingrhythms.co) events, in which people gather together to individually read a book of their choice, to curated background music; turning reading into a communal experience.

### Audiobooks

* [Why Can't I Just Enjoy Things?: A Comedian's Guide to Autism](https://www.goodreads.com/book/show/204881333-why-can-t-i-just-enjoy-things) (Novellie, 2024)
* [Friends: Understanding the Power of our Most Important Relationships](https://www.goodreads.com/book/show/56883977-friends) (Dunbar, 2021)
* [Move: The Forces Uprooting Us](https://www.goodreads.com/book/show/56898242-move) (Khanna, 2021)
* [The Body Keeps the Score: Brain, Mind, and Body in the Healing of Trauma](https://www.goodreads.com/book/show/18693771-the-body-keeps-the-score) (van der Kolk, 2014)
* [Meditations](https://www.goodreads.com/book/show/30659.Meditations) (Aurelius, 180)
* [How to Become Famous: Lost Einsteins, Forgotten Superstars, and How the Beatles Came to Be](https://www.goodreads.com/book/show/192724314-how-to-become-famous) (Beyer, 2024)

### Papers

* Vaswani, A. et al. (2017). [Attention Is All You Need](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html). _Advances in Neural Information Processing Systems_.
* Sculley, D., et al. (2015). [Hidden technical debt in Machine learning systems](https://dl.acm.org/doi/10.5555/2969442.2969519). In _Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2 (NIPS'15). MIT Press, Cambridge, MA, USA, 2503–2511_. 
* Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). [Dropout: a simple way to prevent neural networks from overfitting](https://jmlr.org/papers/v15/srivastava14a.html). _The journal of machine learning research, 15(1), 1929-1958_.
* Chen, H., Gilad-Bachrach, R., Han, K. et al. [Logistic Regression over Encrypted Data from Fully Homomorphic Encryption](https://doi.org/10.1186/s12920-018-0397-z). _BMC Med Genomics 11 (Suppl 4), 81 (2018)_.
* T. Li, A. K. Sahu, A. Talwalkar and V. Smith (2020). [Federated Learning: Challenges, Methods, and Future Directions](https://doi.org/10.1109/MSP.2020.2975749). _IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 50-60, May 2020_.
* Rigaki, Maria, and Sebastian Garcia. (2023). [A Survey of Privacy Attacks in Machine Learning](https://doi.org/10.1145/3624010). _ACM Computing Surveys, vol. 56, no. 4, Nov. 2023, pp. 1–34_. 
