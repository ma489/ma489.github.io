<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-12-30T20:06:40-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Mansour Ahmed</title><subtitle>Mansour Ahmed&apos;s Personal Blog</subtitle><author><name>Mansour Ahmed</name><email>mail@mansourahmed.com</email></author><entry><title type="html">2024 Reading List</title><link href="http://localhost:4000/2024/12/2024-reading-list/" rel="alternate" type="text/html" title="2024 Reading List" /><published>2024-12-20T07:02:00-05:00</published><updated>2024-12-20T07:02:00-05:00</updated><id>http://localhost:4000/2024/12/2024-reading-list</id><content type="html" xml:base="http://localhost:4000/2024/12/2024-reading-list/"><![CDATA[<p>As 2024 draws to a close, it’s time for another retrospective list of the most interesting books I’ve read this year.</p>

<p>The list is decidedly shorter than usual, a result of my focus on completing my Master’s degree — a year spent diving into academic papers more than leisure reading. (I’ve not included the papers I read for my thesis here).</p>

<p>Interestingly, not a single physical book made the cut this year — a sign of the times, perhaps? Additionally, most of the titles are <em>recent</em> publications, with the obvious exception of the timeless wisdom found in Marcus Aurelius’ writings (!).</p>

<p>Finally, a delightful addition to my reading routine this year was to attend the excellent <a href="https://readingrhythms.co"><em>Reading Rhythms</em></a> events, in which people gather together to individually read a book of their choice, to curated background music; turning reading into a communal experience.</p>

<h3 id="audiobooks">Audiobooks</h3>

<ul>
  <li><a href="https://www.goodreads.com/book/show/204881333-why-can-t-i-just-enjoy-things">Why Can’t I Just Enjoy Things?: A Comedian’s Guide to Autism</a> (Novellie, 2024)</li>
  <li><a href="https://www.goodreads.com/book/show/56883977-friends">Friends: Understanding the Power of our Most Important Relationships</a> (Dunbar, 2021)</li>
  <li><a href="https://www.goodreads.com/book/show/56898242-move">Move: The Forces Uprooting Us</a> (Khanna, 2021)</li>
  <li><a href="https://www.goodreads.com/book/show/18693771-the-body-keeps-the-score">The Body Keeps the Score: Brain, Mind, and Body in the Healing of Trauma</a> (van der Kolk, 2014)</li>
  <li><a href="https://www.goodreads.com/book/show/30659.Meditations">Meditations</a> (Aurelius, 180)</li>
  <li><a href="https://www.goodreads.com/book/show/192724314-how-to-become-famous">How to Become Famous: Lost Einsteins, Forgotten Superstars, and How the Beatles Came to Be</a> (Beyer, 2024)</li>
</ul>

<h3 id="papers">Papers</h3>

<ul>
  <li>Vaswani, A. et al. (2017). <a href="https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">Attention Is All You Need</a>. <em>Advances in Neural Information Processing Systems</em>.</li>
  <li>Sculley, D., et al. (2015). <a href="https://dl.acm.org/doi/10.5555/2969442.2969519">Hidden technical debt in Machine learning systems</a>. In <em>Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2 (NIPS’15). MIT Press, Cambridge, MA, USA, 2503–2511</em>.</li>
  <li>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). <a href="https://jmlr.org/papers/v15/srivastava14a.html">Dropout: a simple way to prevent neural networks from overfitting</a>. <em>The journal of machine learning research, 15(1), 1929-1958</em>.</li>
  <li>Chen, H., Gilad-Bachrach, R., Han, K. et al. <a href="https://doi.org/10.1186/s12920-018-0397-z">Logistic Regression over Encrypted Data from Fully Homomorphic Encryption</a>. <em>BMC Med Genomics 11 (Suppl 4), 81 (2018)</em>.</li>
  <li>T. Li, A. K. Sahu, A. Talwalkar and V. Smith (2020). <a href="https://doi.org/10.1109/MSP.2020.2975749">Federated Learning: Challenges, Methods, and Future Directions</a>. <em>IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 50-60, May 2020</em>.</li>
  <li>Rigaki, Maria, and Sebastian Garcia. (2023). <a href="https://doi.org/10.1145/3624010">A Survey of Privacy Attacks in Machine Learning</a>. <em>ACM Computing Surveys, vol. 56, no. 4, Nov. 2023, pp. 1–34</em>.</li>
</ul>]]></content><author><name>Mansour Ahmed</name><email>mail@mansourahmed.com</email></author><summary type="html"><![CDATA[As 2024 draws to a close, it’s time for another retrospective list of the most interesting books I’ve read this year.]]></summary></entry><entry><title type="html">Master’s Thesis: Spatial Regression with Deep Learning and Attention</title><link href="http://localhost:4000/2024/09/thesis-deep-learning-attention-spatial-regression/" rel="alternate" type="text/html" title="Master’s Thesis: Spatial Regression with Deep Learning and Attention" /><published>2024-09-20T08:02:00-04:00</published><updated>2024-09-20T08:02:00-04:00</updated><id>http://localhost:4000/2024/09/thesis-dl-spatial</id><content type="html" xml:base="http://localhost:4000/2024/09/thesis-deep-learning-attention-spatial-regression/"><![CDATA[<p>I am excited to share that I have successfully completed and submitted my <strong>Master’s thesis</strong>, fulfilling the final requirement for my degree in <strong>Machine Learning and Data Science</strong> at <a href="https://www.imperial.ac.uk">Imperial College London</a>.</p>

<p>At a high-level, the project focused on using Deep Learning to predict property prices (with <a href="https://www.tensorflow.org/">TensorFlow</a>). I was supervised by the inspiring, highly knowledgable and supportive Dr. J Martin. The thesis is titled:</p>

<blockquote>
  <p><strong>Predicting Property Prices in New York City: Deep Learning with Attention for Spatial Regression on Areal Data</strong>.</p>
</blockquote>

<p>Below, you’ll find the abstract. If you’re interested in reading the full text (~10,000 words), feel free to reach out via email — I’d be happy to share a copy!</p>

<h2 id="abstract">Abstract</h2>

<p>        Accurately predicting property prices is a challenging but important problem. An
accurate understanding of property prices is important both as guidance to individuals
(buyers gauging a fair price, and sellers evaluating their properties), and as signals of
broader economic trends. <br />
        This thesis approaches the task of property price prediction as a spatial regression problem. Specifically, we investigate the efficacy of deep learning with attention mechanisms in the task of spatial regression, as applied to a city-scale areal dataset. This methodological exploration is conducted with particular application to predicting residential property prices in New York City (NYC), at the city block level. Residential property prices in NYC are among the most expensive in the world, and so accurate property price prediction has significant financial implications. <br />
        A comprehensive exploratory data analysis was undertaken to uncover the complexities of the NYC residential property market. Through the use of modern deep learning techniques, and statistical methods including Bayesian optimisation and spatial cross-validation, multiple models were developed and rigorously evaluated, and their performances compared. <br />
        We propose the use of a deep learning model that incorporates an attention mechanism. Empirical results show that this model outperforms traditional approaches such as Geographically Weighted Regression, and demonstrates a more modest performance improvement over deep learning models <em>without</em> attention. Furthermore, inspection of the attention weights of the model offers possible additional interpretability and insight. <br />
        We propose several future directions in this active research area, including the use of multi-head attention and alternative positional encodings, which may offer further improvements in predictive performance.</p>

<p><strong>Keywords</strong>: <em>spatial data, regression, deep learning, attention, property prices</em></p>]]></content><author><name>Mansour Ahmed</name><email>mail@mansourahmed.com</email></author><summary type="html"><![CDATA[I am excited to share that I have successfully completed and submitted my Master’s thesis, fulfilling the final requirement for my degree in Machine Learning and Data Science at Imperial College London.]]></summary></entry><entry><title type="html">Generating Audio from Spoken Digits, with VQ-VAE and LSTM models</title><link href="http://localhost:4000/2024/04/dl-audio-generation/" rel="alternate" type="text/html" title="Generating Audio from Spoken Digits, with VQ-VAE and LSTM models" /><published>2024-04-22T08:02:00-04:00</published><updated>2024-04-22T08:02:00-04:00</updated><id>http://localhost:4000/2024/04/audio-generation-dl</id><content type="html" xml:base="http://localhost:4000/2024/04/dl-audio-generation/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>This post presents an analysis of the <a href="https://github.com/Jakobovski/free-spoken-digit-dataset">Free Spoken Digit Dataset</a> (FSDD) (an “audio/speech dataset consisting of recordings of spoken digits” <a href="#references">[1]</a>) and the use of this dataset in the development of two machine learning models (a <a href="https://papers.nips.cc/paper_files/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html">VQ-VAE</a> model and an <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a> model) which are combined for the purpose of generating new samples of audio of spoken digits.</p>

<h2 id="exploratory-data-analysis">Exploratory Data Analysis</h2>

<p>The FSDD dataset consists of 2500 audio recordings, by 5 speakers, of 10 different digits (where there is 50 recordings per digit per speaker). These recordings are in wav files with a 8kHz sample rate, and are of variable duration.</p>

<p>Figure 1 below shows the audio waveforms of a sample recording for 9 of the 10 digits; we can see that recordings of different digits by different speakers have different shaped waveforms. Figure 2 shows the distribution of durations of the recordings; we can see that most recordings are quite short - indeed 99% are shorter than 0.78 seconds.</p>

<p><img src="/assets/img/audio-generation-fig1-2.png" alt="Fig1-2" class="center-image" /></p>

<p>Audio characteristics of the samples were computed (using <a href="https://librosa.org/doc/latest/index.html"><code class="language-plaintext highlighter-rouge">librosa</code></a>: <a href="https://en.wikipedia.org/wiki/Zero-crossing_rate">Zero-Crossing Rate</a> (ZCR), <a href="https://en.wikipedia.org/wiki/Spectral_centroid">Spectral Centroid</a>, <a href="https://en.wikipedia.org/wiki/Bandwidth_(signal_processing)">Spectral Bandwidth</a>, <a href="https://librosa.org/doc/main/generated/librosa.feature.spectral_rolloff.html">Spectral Rolloff</a>, and <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">Mel-frequency cepstral coefficients</a> (MFCC). Figure 3 below summarises the means of these values for each digit. We can see for example that the digit ‘6’ has a distinctive mean ZCR value that is much higher than for other digits. Similarly, we see that the digit ‘5’ has a mean Spectral Rolloff value that is much lower than the other digits.</p>

<p><img src="/assets/img/audio-generation-fig3.png" alt="Fig3" class="center-image" /></p>

<h2 id="modelling--results">Modelling &amp; Results</h2>

<p>The goal was to develop models for the purpose of generating new samples of audio of spoken digits. To achieve this, two models were developed and their functionality combined into one system for generating new audio samples. These two models are referred to as model <em>A</em> and model <em>B</em>, respectively, and are described in detail below.</p>

<p>At a high-level: model <em>A</em> (Figure 4) is a <em>Vector-Quantized Variational Autoencoder</em> (VQ-VAE), similar to that introduced in <a href="#references">[2]</a>, and is used to learn efficient encodings of the audio data. Model <em>B</em> (Figure 5) is an LSTM model that is trained on model <em>A</em>’s output (indeed, one LSTM per VQ codebook), and is used to generate new encoding sequences; these sequences are in turn reconstructed into listenable audio clips using model <em>A</em>’s decoder. Figures 6 and 7 below show the training and inference processes of the combined model system.</p>

<p><img src="/assets/img/audio-generation-fig4-7.png" alt="Fig4-7" class="center-image" /></p>

<h3 id="model-a-vq-vae">Model A: VQ-VAE</h3>

<p>The FSDD data was processed by first extracting the audio component (for each clip, this is a 1-D array representing amplitude over time), then filtering out very long and short clips, then zero-padding the remaining clips to 1 second duration (uniform size to simplify code). The data was then split into training and test sets, and finally shuffled and batched.</p>

<p>The training data was used to train the ‘VQ-VAE’ model. The model has 3 components:</p>
<ul>
  <li>An encoder comprising three 1-D convolutional layers</li>
  <li>An RVQ-VAE layer comprising Nq VQ-VAE layers</li>
  <li>A decoder comprising three 1-D convolutional transpose layers</li>
</ul>

<p>The model was trained using customised training logic that optimised for loss defined as loss = reconstruction loss + RVQVAE loss.</p>

<p>Various settings for the hyperparameters were explored as part of experimentation, specifically:</p>
<ul>
  <li>The hyperameter Nq that controls the number of codebooks in the RVQ-VAE layer (values: 5, 10, 50).</li>
  <li>The number of epochs that training is conducted for (values: 30, 100, 500).</li>
</ul>

<p>Higher Nq values and extended training epochs resulted in improved outcomes; however, this led to longer training times. To assess the results, a combination of training loss comparisons and subjective manual auditory evaluations was used (audio samples can be found in the notebook). The optimal combination achieved an overall loss of 0.0761 (reconstruction loss of 0.0349 and an RVQVAE loss of 0.0412). With a <a href="https://www.nvidia.com/en-us/data-center/tesla-t4/">Nvidia T4 GPU</a> on <a href="https://colab.research.google.com/">Google Colab</a>, this setup took approximately 1.5 hours to train. Figures 8 and 9 compare the loss curves for two sets of hyperparameter settings, showing that higher Nq values result in lower loss in fewer epochs.</p>

<p><img src="/assets/img/audio-generation-fig8-9.png" alt="Fig8-9" class="center-image" /></p>

<h3 id="model-b-lstm-model">Model B: LSTM model</h3>

<p>The audio training data was once again processed by (forward) passing it through the trained VQ-VAE model’s encoder and RVQ-VAE layers (notably, not the decoder) to generate the corresponding Nq encodings (one for each codebook in the RVQ-VAE). These encodings were then used to train Nq LSTM models.</p>

<p>Each LSTM model comprises:</p>
<ul>
  <li>An embedding layer with an input dimension of 64 (same as the codebook vectors), and an output
dimension of 64</li>
  <li>Two LSTM layers with 256 units</li>
  <li>A Dense output layer with softmax activation that outputs probabilities over the possible tokens (additional logic later chooses the token with the highest probability)</li>
</ul>

<p>The model was trained using the standard tensorflow fit method and training logic, with 10% of the training data used as a validation set. Various settings for the hyperparameters were explored as part of experimentation, including a different number of LSTM layers and different dropout rates (ultimately 2 and 0.2 was chosen, respectively), while trading off against training duration.</p>

<p>Performance was measured using training and validation loss and accuracy. While performance against training/validation was good (~70% accuracy across the LSTM models), the performance against the unseen test set was not good: it was found that the model outputted constant vectors of the same code. This unfortunately leads to poor quality reconstructed audio. With more time, this poor performance could be investigated.</p>

<h2 id="future-work">Future Work</h2>

<p>With more time, future work in this direction could involve:</p>

<ol>
  <li>Explore the use of representing audio as an image instead of a series of amplitude values over time, possibly by combining graphical representations of audio characteristics (i.e. spectrograms and MFCCs) into an image. Modify the encoder and decoder of the VQ-VAE to use 2-D convolutional layers accordingly. This was partially attempted in the notebook, but futher implementation is needed to convert the reconstructed image representation back to audio (this may be challenging).</li>
  <li>Train both models for more epochs to achieve better performance (lower loss and higher accuracy). Due to time limitations and available resources, training was conducted for fewer epochs than desired.</li>
  <li>Investigate and optimise the hyperparameters of both models, especially the LSTM (model Q3b). Conduct more experimentation to find the optimal settings for improved performance.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>In conclusion, this report demonstrates the feasibility of generating new samples of audio data by first training a VQ-VAE model to learn efficient codings of the audio data, and then using those encodings to train an LSTM model to generate new encodings, which can in turn be decoded into listenable audio. While the final audio clips generated are not of the highest quality, the report details possible directions for future work to pursue better performance.</p>

<hr />

<h2 id="references">References</h2>

<ol>
  <li>Zohar J., et al. (2018). <a href="https://doi.org/10.5281/zenodo.1342401">Jakobovski/free-spoken-digit-dataset: v1.0.8 (v1.0.8)</a>. Zenodo.</li>
  <li>van den Oord, A., Vinyals, O. &amp; Kavukcuoglu, K. (2017). <a href="https://papers.nips.cc/paper_files/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html">Neural discrete representation learning</a>. <em>Proceedings of the 31st International Conference on Neural Information Processing Systems, 6309-6318</em>.</li>
</ol>

<hr />

<p>Note: This post is based on a report written for an assignment as part of my degree in <em>Machine Learning &amp; Data Science</em> at <a href="https://www.imperial.ac.uk">Imperial College London</a>. The assignment was part of <em>Deep Learning</em> module taught by the highly knowledgable and supportive Dr. K Webster (all credit due for the assignment setting, the goals and objectives of the modelling, and the selection and provision of the dataset).</p>]]></content><author><name>Mansour Ahmed</name><email>mail@mansourahmed.com</email></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">2023 Reading List</title><link href="http://localhost:4000/2023/12/2023-reading-list/" rel="alternate" type="text/html" title="2023 Reading List" /><published>2023-12-28T07:02:00-05:00</published><updated>2023-12-28T07:02:00-05:00</updated><id>http://localhost:4000/2023/12/2023-reading-list</id><content type="html" xml:base="http://localhost:4000/2023/12/2023-reading-list/"><![CDATA[<p>Another year, another journey through a fascinating set of books. Some of my selections this year were inspired by the excellent <a href="https://www.rebelbook.club/"><em>Rebel Book Club</em></a>, a community I’m proud to be part of. Others were part of my ongoing Master’s degree studies in Machine learning. Explore my curated list below!</p>

<h3 id="books">Books</h3>

<ul>
  <li><a href="https://www.statlearning.com/">An Introduction to Statistical Learning (with Applications in R)</a> (James et al, 2013)</li>
  <li><a href="https://hastie.su.domains/ElemStatLearn/">The Elements of Statistical Learning (Data Mining, Inference, and Prediction)</a> (Hastie et al, 2009)</li>
</ul>

<h3 id="audiobooks">Audiobooks</h3>

<ul>
  <li><a href="https://www.goodreads.com/book/show/90590134-the-coming-wave">The Coming wave: Technology, Power, and the Twenty-first Century’s Greatest Dilemma</a> (Suleyman, 2023)</li>
  <li><a href="https://www.goodreads.com/book/show/48816586-software-engineering-at-google">Software Engineering at Google</a> (Winters et al, 2020)</li>
  <li><a href="https://www.goodreads.com/book/show/38315.Fooled_by_Randomness">Fooled by Randomness: The Hidden Role of Chance in Life and in the Markets</a> (Taleb, 2001)</li>
  <li><a href="https://www.goodreads.com/book/show/84525.What_Got_You_Here_Won_t_Get_You_There">What Got You Here Won’t Get You There</a> (Goldsmith, 2006)</li>
  <li><a href="https://www.goodreads.com/book/show/66933.The_Wretched_of_the_Earth">Wretched of the Earth</a> (Fanon, 1961)</li>
  <li><a href="https://www.goodreads.com/book/show/1303.The_48_Laws_of_Power">The 48 Laws of Power</a> (Greene, 1998)</li>
  <li><a href="https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow">Thinking, Fast and Slow</a> (Kahneman, 2011)</li>
  <li><a href="https://www.goodreads.com/book/show/42731728-lost-in-a-good-game">Lost in a Good Game: Why we play video games and what they can do for us</a> (Etchells, 2019)</li>
  <li><a href="https://www.goodreads.com/book/show/55338982-cultish">Cultish: The Language of Fanaticism</a> (Montell, 2021)</li>
  <li><a href="https://www.goodreads.com/book/show/45415787-the-unexpected-joy-of-the-ordinary">The Unexpected Joy of the Ordinary</a> (Gray, 2020)</li>
  <li><a href="https://www.goodreads.com/book/show/27491.The_Evolution_Of_Desire">The Evolution of Desire</a> (Buss, 1994)</li>
</ul>

<h3 id="papers">Papers</h3>

<ul>
  <li>Fred S. Guthery and Ralph L. Bingham. (2007). <a href="https://doi.org/10.2193/2006-285">A Primer on Interpreting Regression Models</a>. <em>Journal of Wildlife Management 71(3), 684-692, (1 May 2007)</em>.</li>
  <li>Amy Berrington de González. D. R. Cox. (2007). <a href="https://doi.org/10.1214/07-AOAS124">Interpretation of interaction: A review</a>. <em>Ann. Appl. Stat. 1 (2) 371 - 385, December 2007</em>.</li>
  <li>Chesnaye, N. C., et. al. (2022). <a href="https://doi.org/10.1093/ckj/sfab158">An introduction to inverse probability of treatment weighting in observational research</a>. <em>Clinical Kidney Journal, Volume 15, Issue 1, January 2022, Pages 14–20</em>.</li>
  <li>Kohavi, R., Tang, D., &amp; Xu, Y. (2020). <a href="http://doi.org/10.1017/9781108653985.012">Ethics in Controlled Experiments</a>. In <em>Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing (pp. 116-124)</em>. Cambridge: Cambridge University Press.</li>
  <li>Van Der Bles, A. M., et al. (2019). <a href="http://doi.org/10.1098/rsos.181870">Communicating uncertainty about facts, numbers and science</a>. <em>R. Soc. Open Sci. 6: 181870</em>.</li>
  <li>Wickham, H. (2014). <a href="https://doi.org/10.18637/jss.v059.i10">Tidy Data</a>. <em>Journal of Statistical Software, 59(10), 1–23</em>.</li>
  <li>Wickham, H. (2010). <a href="https://doi.org/10.1198/jcgs.2009.07098">A Layered Grammar of Graphics</a>. <em>Journal of Computational and Graphical Statistics, 19(1), 3–28</em>.</li>
  <li>Watson, J., &amp; Holmes, C. (2016). <a href="http://www.jstor.org/stable/26408074">Approximate Models and Robust Decisions</a>. <em>Statistical Science, 31(4), 465–489</em>.</li>
  <li>Meng, X.-L. (2018). <a href="https://www.jstor.org/stable/26542550">Statistical Paradises and Paradoxes in Big Data (I): Law of Large Populations, Big Data Paradox, and the 2016 US Presidential Election</a>. <em>The Annals of Applied Statistics, 12(2), 685–726</em>.</li>
  <li>Lindley, D. V., &amp; Phillips, L. D. (1976). <a href="https://doi.org/10.1080/00031305.1976.10479154">Inference for a Bernoulli Process (a Bayesian View)</a>. <em>The American Statistician, 30(3), 112–119</em>.</li>
  <li>Paleyes, A., Urma, R. G., &amp; Lawrence, N. D. (2022). <a href="https://dl.acm.org/doi/full/10.1145/3533378">Challenges in deploying machine learning: a survey of case studies</a>. <em>ACM computing surveys, 55(6), 1-29</em>.</li>
  <li>Kim, M., Zimmermann, T., DeLine, R., &amp; Begel, A. (2017). <a href="https://ieeexplore.ieee.org/abstract/document/8046093">Data Scientists in Software Teams: State of the Art and Challenges</a>. <em>IEEE Transactions on Software Engineering, 44(11), 1024-1038</em>.</li>
  <li>Verma, S., &amp; Rubin, J. (2018). <a href="https://dl.acm.org/doi/abs/10.1145/3194770.3194776">Fairness Definitions Explained</a>. <em>In Proceedings of the international workshop on software fairness (pp. 1-7)</em>.</li>
</ul>]]></content><author><name>Mansour Ahmed</name><email>mail@mansourahmed.com</email></author><summary type="html"><![CDATA[Another year, another journey through a fascinating set of books. Some of my selections this year were inspired by the excellent Rebel Book Club, a community I’m proud to be part of. Others were part of my ongoing Master’s degree studies in Machine learning. Explore my curated list below!]]></summary></entry><entry><title type="html">Multiclass Image Classification with CNNs</title><link href="http://localhost:4000/2023/12/cnn-image-classification/" rel="alternate" type="text/html" title="Multiclass Image Classification with CNNs" /><published>2023-12-21T07:02:00-05:00</published><updated>2023-12-21T07:02:00-05:00</updated><id>http://localhost:4000/2023/12/cnn-image-classification</id><content type="html" xml:base="http://localhost:4000/2023/12/cnn-image-classification/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In this post, I’ll present the analysis and processing of an images dataset, and the training and application of <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Network</a> (CNN) models (using <a href="https://pytorch.org/">PyTorch</a>) for the purpose of <a href="https://en.wikipedia.org/wiki/Multiclass_classification"><em>multiclass classification</em></a> of the dataset. The dataset is the ‘The Simpsons Characters Dataset’ <a href="#references">[1]</a> (obtained from Kaggle <a href="#references">[1]</a>), and contains still images of different characters taken from the television show <em>The Simpsons</em> <a href="#references">[2]</a>.</p>

<p>The goal of the analysis and modelling, and hence the problem statement, is: <strong>Given the Simpsons Characters image dataset, how accurately can the characters in the images be correctly identified?</strong></p>

<p>The accompanying code and data repository can be found <a href="https://github.com/ma489/mlds-uda-hw4">here</a>.</p>

<h2 id="the-data">The Data</h2>

<h3 id="background--overview">Background &amp; Overview</h3>

<p>The full image dataset contains ~22,000 still images of 42 different cartoon characters. The images are taken from the popular, animated, American television show The Simpsons <a href="#references">[2]</a>. As of 2023, the show has broadcast 760 episodes across 35 ‘seasons’ since 1989 <a href="#references">[3]</a>. Over this time, (human) fans of the television show have become familiar with the characters and can easily identify them. Figures 1-6 below show example images from the dataset, with the captions indicating the respective character’s name. Each image in the dataset contains only one character.</p>

<p>For the purposes of this analysis, a sub-sample of the data was selected. This is because, given its size, using the full dataset presents a computational challenge: training a model on it would take impractically long. Instead a sub-sample of ~⅓ of the size of the full sample was taken. This subsample contains <strong>8,363 images of 6 characters</strong>.</p>

<p>These characters are (short names in brackets): Homer Simpson (<em>Homer</em>), Marge Simpson (<em>Marge</em>), Bart Simpson (<em>Bart</em>), Lisa Simpson (<em>Lisa</em>), Krusty the Clown (<em>Krusty</em>) and Apu Nahasapeemapetilon (<em>Apu</em>). These 6 characters are the same ones shown in Figures 1-6. Further detail on this sub-sampling is described in the <a href="#pre-processing">Pre-Processing</a> section. Note: in the Github repo, the dataset was further sub-sampled to reduce the size to under 100mb; again, this is described in the Pre-Processing section.</p>

<p><img src="/assets/img/simpsons_characters_fig1-6.png" alt="Fig1-6" class="center-image" /></p>

<h3 id="dataset-challenges">Dataset Challenges</h3>

<p>There are a number of challenges to working with this dataset, which fall into two categories: (1) complexity due to the quality of the images, and (2) intrinsic complexity of the content of the images.</p>

<p>(1) The quality of the images introduces complexity:</p>

<ul>
  <li>Firstly, the images vary in both size and resolution. For example, Figure 7 shows a low resolution noisy image (288 × 432 pixels, 72 pixels/inch).</li>
  <li>Secondly, there is varying character cropping and centering. For example, Figure 8 shows the character is partially cropped, possibly losing useful information. In Figure 9 the character is not centred, and so there is possibly a lot of irrelevant information included in the image.</li>
</ul>

<p>(2) There is intrinsic complexity to the content of the images:</p>

<ul>
  <li>For example, there is significant variation between images: the characters change outfits, or the characters appear against varying backgrounds (compare Figure 7 with Figure 9).</li>
  <li>There are also similarities that may make it difficult to discern between characters: the most obvious of which is the show’s hallmark animation style wherein most characters are yellow skin tone and circular eyes <a href="#references">[3]</a> (consider Figures 1, 2, 3 and 4).</li>
</ul>

<p><img src="/assets/img/simpsons_marge_fig7-9.png" alt="Fig7-9" class="center-image" /></p>

<h2 id="pre-processing">Pre-Processing</h2>

<p>The full dataset comprises two sets of images:</p>

<ul>
  <li>Training set: ~21,000 images of 42 characters</li>
  <li>Test set: ~1,000 images of 20 characters (a subset of the characters in the training set)</li>
</ul>

<p>The data was already mostly organised into a training and test set, however some movement and manipulation of the filenames and directories was required in order to facilitate loading and processing by the Pytorch library <a href="#references">[6]</a>. One such shell script for performing these file operations is included in the Github submission for completeness (<code class="language-plaintext highlighter-rouge">script_to_move_and_rename_test_data_files.sh</code>) .</p>

<p>As mentioned in the Data <a href="#background--overview">Background &amp; Overview</a> section, a subsample of this dataset was used for this analysis. The sub-sample was taken by selecting 6 characters, and using all of their training and test images. The <em>analysis</em> sub-sample thus comprises:</p>

<ul>
  <li>Training Set: 8,063 images of 6 characters</li>
  <li>Test Set: 300 images of 6 characters</li>
</ul>

<p>The 6 characters in the sub-sample were selected in following way, and represent an interesting subset of characters:</p>

<ul>
  <li>
    <p><em>Homer, Marge, Bart,</em> and <em>Lisa</em> were selected as they are all important characters from the show’s eponymous, central family <em>The Simpsons</em>. Additionally, analysis by Schneider (2016) <a href="#references">[4]</a> (see Figure 10) shows that they are also the 4 top characters in the show based on number of words spoken.</p>
  </li>
  <li><em>Krusty</em> was selected as he is also a top-10 character <a href="#references">[4]</a> and, more interestingly, this character’s appearance strongly resembles that of Homer. This was a deliberate design decision by the show’s creators <a href="#references">[7]</a>. Classifying two characters that resemble each other might provide an interesting challenge to the model.</li>
  <li><em>Apu</em> was selected as he is one of the show’s few characters who does not have ‘yellow’ skin tone (a hallmark of the the show’s animation style). Again, this character’s unique skin tone might provide an interesting challenge to model, and give some insight into which image features the model learns from.</li>
</ul>

<p><img src="/assets/img/simpsons_fig10.png" alt="Fig10" class="center-image" /></p>

<p>Figure 11 shows the distribution of character images in the training set. The <em>Training set</em> exhibits some class imbalance (e.g. Homer has ~4x as many images as Apu). In contrast, the <em>Test set</em> is balanced: 50 images each for the 6 characters.</p>

<p>Note that in the final accompany Github repo, further subsampling was performed to bring
the submitted dataset to under 100mb in size (<code class="language-plaintext highlighter-rouge">image_data_subsample_subset.zip</code>) (Training Set: 2,980 images, Test Set: 300 images). This ‘sub-sub-sample‘ contains images of the same 6 characters.</p>

<p>Note however that the analysis in this post uses the analysis subset of 8363 images, and not the Github repo sub-sub-sample.</p>

<p><img src="/assets/img/simpsons_fig11.png" alt="Fig11" class="center-image" /></p>

<p>The images in the dataset were pre-processed in 4 different ways; those 4 approaches are described below:</p>

<ol>
  <li>Pre-processing approach 1 <em>‘Vanilla’</em>: Resize all the images to 255x342 pixels, keeping the 4x3 (television broadcast) aspect ratio.</li>
  <li>Pre-processing approach 2 <em>‘Bounding Box Crop’</em>: Apply pre-processing approach 1, plus: Crop the images using the bounding box coordinates in the ’annotations’ file supplied in the dataset. One of the bounding box coordinates was found to be erroneous and was removed.</li>
  <li>Pre-processing approach 3 <em>‘Denoise’</em>: Apply pre-processing approach 1, plus: Denoise the image using the denoise tv chambolle function from skimage <a href="#references">[8]</a>.</li>
  <li>Pre-processing approach 4 <em>‘Super Resolution’</em>: Apply pre-processing approach 1, plus: Improve the image resolution by using the ninasr b0 ‘super-resolution’ neural network model from the torchSR package <a href="#references">[9]</a>.</li>
</ol>

<p>As a final step in all the pre-processing approaches, all of the images are converted to <code class="language-plaintext highlighter-rouge">Pytorch</code> tensors, for inputting to the model.</p>

<p>To implement the pre-processing steps described above, a custom Pytorch <code class="language-plaintext highlighter-rouge">ImageFolder</code> class was defined <code class="language-plaintext highlighter-rouge">ImageFolderWithPath</code>, which not only loads the image but also captures the path name (the paths include the character names which are used as the class names in the classification model), and applies the pre-processing transformations.</p>

<p>Figures 12-15 below show the results of the different processing approach on the same image.</p>

<p><img src="/assets/img/simpsons_fig12-15.png" alt="Fig12-15" class="center-image" /></p>

<h2 id="modelling">Modelling</h2>

<p>Restating the problem statement: <em>Given the Simpsons Characters image dataset, how accurately can the characters in the images be correctly identified?</em>. This is formulated here as a multi-class classification problem, where the character names are the classes, and the images are to be classified as belonging to a particular class if the image depicts that particular character. Each image contains only one character.</p>

<p>To perform the classification task, a Convolutional Neural Network (CNN) was constructed, using pytorch. CNNs are a popular deep learning technique with many applications but are particularly useful in image classification tasks. A CNN was developed with the following layers:</p>

<ul>
  <li><em>Input layer</em>: Images are inputted as tensors (following pre-processing)</li>
  <li><em>Layer 1 (hidden layer)</em>: Applies: a 2D convolution over the image, followed by <a href="https://en.wikipedia.org/wiki/Batch_normalization">Batch Normalization</a>, followed by a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> (rectified linear unit) function, and finally 2D max <a href="https://en.wikipedia.org/wiki/Pooling_layer">pooling</a>.</li>
  <li><em>Layer 2 (hidden layer)</em>: The same as layer 1, but on the output from layer 1</li>
  <li><em>Fully connected layer</em>: Applies a linear transformation.</li>
  <li><em>Dropout</em>: Applies the <a href="https://en.wikipedia.org/wiki/Dilution_(neural_networks)">dropout</a> regularisation technique, with probability 0.5.</li>
</ul>

<p>As an illustration, Figures 16-18 below show an example original input image, and the intermediate outputs of the network’s hidden layers. (This was taken from ‘Vanilla’ model described below).</p>

<p><img src="/assets/img/simpsons_fig16-18.png" alt="Fig16-18" class="center-image" /></p>

<p>The image dataset was processed separately in each the 4 ways described in the Pre-Processing section, and then inputted into 4 separate CNNs. Thus, there are 4 models that all have the same model architecture but were trained on input data pre-processed in 4 different ways:</p>

<ol>
  <li>Model 1: Referred to as ‘Vanilla’. This model was inputted with the image dataset where only basic pre-processing applied, e.g. resize but keep 4x3 ratio, converting the images to a tensors (‘Pre-processing Approach 1’).</li>
  <li>Model 2: Referred to as ‘Bounding Box Crop’. This model was inputted with the image dataset with all of the same basic pre-processing steps as Model 1, plus the images were cropped using the bounding box annotations supplied with the dataset (‘Pre-processing Approach 2’).</li>
  <li>Model 3: Referred to as ‘Denoise’. This model was inputted with the image dataset with all of the same basic pre-processing steps as Model 1, plus the images were denoised (‘Pre-processing Approach 3’).</li>
  <li>Model 4: Referred to as ‘Super Resolution’. This model was inputted with the image dataset with all of the same basic pre-processing steps as Model 1, plus the resolution of the images was enhanced using a super resolution model (‘Pre-processing Approach 4’).</li>
</ol>

<p>For each of the models: The training data was loaded in batches of 100, and training was performed for 5 epochs over the training data. The number of epochs was chosen to be 5 as to provide a balance between model performance (with respect to test accuracy) and to complete training within a practical length of time.</p>

<p>Training took approximately 2 hours on a Macbook Pro (2020) laptop with the following specs:</p>

<ul>
  <li>Processor: 2 GHz Quad-Core Intel Core i5</li>
  <li>Graphics: Intel Iris Plus Graphics 1536 MB</li>
  <li>Memory: 16 GB 3733 MHz LPDDR4X</li>
  <li>Operating System: macOS 14.2.1</li>
</ul>

<p>Figures 19-22 show, for each of the models, the training loss over the course of the batches. We can see that they all converged fairly quickly, within ~50 batches. The captions give the training loss for the final batch. Model 4 had the highest Training loss but, as explained in the later <a href="results--interpretation">Results &amp; Interpretation</a> section, generalised well to the Test set and demonstrated a high accuracy and f1-score.</p>

<p><img src="/assets/img/simpsons_fig19-22.png" alt="Fig19-22" class="center-image" /></p>

<h2 id="results--interpretation">Results &amp; Interpretation</h2>

<p>The 4 models were evaluated against a Test set of 300 unseen images: 50 images for each of the 6 characters.<br />
        Table 1 compares the models based on a number classification evaluation metrics: Accuracy, Precision, Recall, and f1-score. Micro averages are provided for those latter 3 metrics - since there is no class imbalance in the test data and all classes are of equal importance. The numbers are given to 2 d.p. We can see that Models 1 and 4 performed the best across the metrics: Model 4 had the highest Accuracy and f1-score (both 0.81), Model 1 had the highest Precision (0.84), and both models had equal Recall (0.8). In contrast, Models 2 and 3 performed less well, and specifically Model 2 performed the worst across all metrics.<br />
        The relative poor performance of Models 2 and 3 suggests that removing perceived (but not actually) irrelevant information from an image (Model 2 - bounding box cropping), or removing noise (Model 3 - denoising) may in fact harm the quality of the image and information therein, and thus affect model performance.<br />
        The slightly improved Accuracy and f1-score of Model 4 suggests that improving the resolution of the images can improve model performance, however the improvement is small and so may not justify the increased model training time.<br />
        Table 2 shows the metrics for the Github subset (included here for completeness). We can see that the models’ performance is worst on this subset than for the larger sub-sample - which may be explained by having less data to train on and learn from. However the relative performances of the models remains similar: Model 3 is the worst, and Model 1 and Model 4 perform similarly well.</p>

<p><img src="/assets/img/simpsons_table1-2.png" alt="Table1-2" class="center-image" /></p>

<p>Figures 23-26 show the confusion matrices for the each of the models.<br />
        Across all 4 models, it appears that the character Apu is the most misclassified. This is interesting: Apu is unique among the characters in the dataset in that he is the only one without (mostly) yellow skin tone, which <em>intuitively</em> might suggest that correctly classifying an image of him would be easier. However, this colour information may have been missed/lost by the model (e.g. see Figures 17 and 18 for intermediate model outputs), thus reducing the ability to discern his appearance without that information.<br />
        In contrast, Marge was the least misclassified character across all models - perhaps the unique shape of the character’s head/hair facilitates classification.<br />
        We can also see that Krusty was (as expected) occasionally misclassified as Homer by all of the models, however this was not the most common misclassification: e.g. Model 2 misclassified Krusty as Apu more times than it did Krusty as Homer (13 vs 8 times). This suggests that the animator’s intended similarity between Homer and Krusty did not dominate the features that the models learned.</p>

<p><img src="/assets/img/simpsons_fig23-26.png" alt="Fig23-26" class="center-image" /></p>

<p>The results of this analysis can be compared to the results from another project by the dataset’s curator (Attia 2017) <a href="#references">[5]</a>. In that project, the f1-score, precision and recall were all 0.9. That was achieved through:</p>
<ol>
  <li>Using the full dataset of 22k images</li>
  <li>Implementing a more sophisticated and complex deep learning neural network architecture with more layers</li>
  <li>Training for more epochs (200 epochs).</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>In conclusion, this analysis and report has shown that, through careful pre-processing of image data and appropriate model selection and training, it is possible to achieve reasonably good performance in the multi-class classification problem of identifying characters in still images taken from the television show <em>The Simpsons</em>.</p>

<p>An accuracy and f1-score of 0.81 were achieved through a combination of applying super-resolution to the images in the pre-processing stage, and training a convolutional neural network with 2 hidden layers for 5 epochs over the data. It was shown that applying image cropping and denoising could have a <em>negative</em> impact on classification performance.</p>

<p>Further analysis could explore how to improve classification performance further, for example by <em>combining</em> these pre-processing techniques, or training the model for more epochs, or adding more layers to the network.</p>

<hr />

<h2 id="references">References</h2>

<ol>
  <li>Attia, A. (2017). <a href="https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset">The Simpsons Characters Dataset</a></li>
  <li>Groening, M., et al. (1989) The Simpsons. Fox Broadcasting Company.</li>
  <li>Wikipedia. (2023) <a href="https://en.wikipedia.org/wiki/The Simpsons">The Simpsons</a>.</li>
  <li>Schneider, T. (2016). <a href="https://toddwschneider.com/posts/the-simpsons-by-the-data/">The Simpsons by the Data</a></li>
  <li>Attia, A. (2017). <a href="https://medium.com/alex-attia-blog/the-simpsons-character-recognition-using-keras-d8e1796eae36">The Simpsons characters recognition and detection using Keras (Part 1)</a>.</li>
  <li>Paszke, A., et al. (2019) Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32.</li>
  <li>Caroll, L. (2007). <a href="https://web.archive.org/web/20071220140402/http://www.mtv.com/movies/news/articles/1565538/20070725/story.jhtml">‘Simpsons’ Trivia (MTV website archived)</a>.</li>
  <li>Van der Walt, S. et al. (2014) scikit-image: image processing in Python. PeerJ, 2, p.e453.</li>
  <li>Gouvine, G. (2021). <a href="https://doi.org/10.5281/zenodo.4868308">torchSR</a>.</li>
</ol>]]></content><author><name>Mansour Ahmed</name><email>mail@mansourahmed.com</email></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Sentiment Analysis with scikit-learn, nltk and word2vec</title><link href="http://localhost:4000/2023/11/sentiment-classification/" rel="alternate" type="text/html" title="Sentiment Analysis with scikit-learn, nltk and word2vec" /><published>2023-11-29T07:02:00-05:00</published><updated>2023-11-29T07:02:00-05:00</updated><id>http://localhost:4000/2023/11/sentiment-classification</id><content type="html" xml:base="http://localhost:4000/2023/11/sentiment-classification/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>This post presents a process of performing <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> the <a href="https://www.kaggle.com/datasets/harshalhonde/starbucks-reviews-dataset"><em>Starbucks Reviews Dataset</em></a> from <a href="https://www.kaggle.com/">Kaggle</a>. The goal was to classify reviews as either Positive or Negative. Two models were developed for this purpose, and their performances compared.</p>

<p>The accompanying code and data repository can be found <a href="https://github.com/ma489/sentiment-classification-starbucks">here</a>.</p>

<h2 id="data">Data</h2>

<p>The ‘Starbucks Reviews Dataset’ contains data from online customer reviews of Starbucks coffee shop locations. The dataset contains 850 samples containing (among other features) a text review and a numeric rating (on a scale of 1-5).</p>

<p>First this data was cleaned up to remove incomplete samples: Only 705 of the 850 samples contained a numeric rating, and of those only 703 also contained an actual text review. This set of 703 reviews and ratings was used for modeling.</p>

<p>The distribution of the numeric ratings is given in Figure 1. We can see that the distribution of ratings is right-skewed (more negative ratings). A modelling choice was made to set a threshold of <em>3 and above</em> to be a <em>Positive</em> rating, and <em>below 3</em> to be a <em>Negative</em> rating - these would be the class labels. Figure 2 shows the resulting distribution of sentiment classes (Positive/Negative) - with a clear class imbalance.</p>

<p>The text reviews were then pre-processed, with the following steps applied to each document:</p>

<ul>
  <li>Remove hyperlinks</li>
  <li>Make all words lower case</li>
  <li>Remove new line characters</li>
  <li>Remove characters that are not whitespace and not in English alphabet</li>
  <li>Remove words that are in the stopwords list provided by the <a href="https://www.nltk.org/"><code class="language-plaintext highlighter-rouge">nltk</code></a> package</li>
  <li>Remove the word ‘Starbucks’</li>
  <li>Perform stemming, using the <code class="language-plaintext highlighter-rouge">PorterStemmer</code> from <a href="https://www.nltk.org/"><code class="language-plaintext highlighter-rouge">nltk</code></a></li>
</ul>

<p><img src="/assets/img/sentiment_classification_hists1-2.png" alt="Fig1" class="center-image" /></p>

<h2 id="modelling">Modelling</h2>

<p>The reviews dataset was used as follows:</p>

<ul>
  <li>The text reviews were vectorized to create the independent variables</li>
  <li>The numeric ratings were converted to sentiment classes, these would be the class labels (the dependent variable ’Y’)</li>
</ul>

<p>The reviews dataset was then split into training and test sets, with a 75%/25% split (with shuffling).</p>

<p>Two models were developed, both based on <code class="language-plaintext highlighter-rouge">RandomForestClassifier</code> from <a href="https://scikit-learn.org/stable/">scikit-learn</a>. A Random Forest Classifer was chosen as it is robust to datasets with class imbalance (as is the case here). Where the two models diverge, however, is in the choice of text vectorization method:</p>
<ul>
  <li>Model 1 uses a <code class="language-plaintext highlighter-rouge">TfidfVectorizer</code></li>
  <li>Model 2 uses a <a href="https://en.wikipedia.org/wiki/Word2vec"><code class="language-plaintext highlighter-rouge">word2vec</code></a> implementation from the <a href="https://radimrehurek.com/gensim/"><code class="language-plaintext highlighter-rouge">gensim</code></a> package.</li>
</ul>

<p>Figure 3 shows a visualisation of one of the decision trees from Model 1, up to depth 5.</p>

<p>Note: when the models are applied to the test set, out-of-vocabulary terms are ignored.</p>

<p><img src="/assets/img/sentiment_classification_model1.png" alt="Fig3" class="center-image" /></p>

<h2 id="results">Results</h2>

<p>The two models were applied to the test set and evaluated. Table 1 compares the models based on a number classification evaluation metrics: Accuracy, Precision, Recall, and f1-score. Macro averages are provided for those latter 3 metrics - since there is a class imbalance in the data, this should be taken into account when evaluating the models’ performance. The first model, <code class="language-plaintext highlighter-rouge">RandomForestClassifer</code> with <code class="language-plaintext highlighter-rouge">TfidfVectorizer</code>, performed better across all metrics. Figures 4 and 5 show the respective confusion matrices.</p>

<p><img src="/assets/img/sentiment_classification_table1fig4-5.png" alt="Fig4-5" class="center-image" /></p>

<hr />

<p>Note: This post is based on a report written for an assignment as part of my degree in <em>Machine Learning &amp; Data Science</em> at <a href="https://www.imperial.ac.uk">Imperial College London</a>. The assignment was part of <em>Unstructured Data Analysis</em> module taught by Dr A. Calissano (all credit due for the assignment setting, the goals and objectives of the modelling, and the selection and provision of the dataset).</p>]]></content><author><name>Mansour Ahmed</name><email>mail@mansourahmed.com</email></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Census Data Classification with PySpark ML</title><link href="http://localhost:4000/2023/07/pyspark-ml-classification/" rel="alternate" type="text/html" title="Census Data Classification with PySpark ML" /><published>2023-07-19T08:01:00-04:00</published><updated>2023-07-19T08:01:00-04:00</updated><id>http://localhost:4000/2023/07/pyspark-ml</id><content type="html" xml:base="http://localhost:4000/2023/07/pyspark-ml-classification/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In this post, I’ll present a statistical analysis of the ‘Census Income’ dataset created by Becker &amp; Kohavi <a href="references">[1]</a>, using the PySpark MLlib package <a href="references">[2]</a>. The dataset was downloaded from the <a href="https://archive.ics.uci.edu/">UCI Machine Learning Repository</a>, and contains various demographic details of ~48k individuals (from the 1996 US Census) and whether their respective income exceeds $50<em>k</em> USD.</p>

<p>The goal of this analysis is to answer the question: <strong>Given US Census demographic data, to what accuracy can the individuals in the dataset be classified as
earning either above or below $50k?</strong>.</p>

<p>The accompanying code and data repository can be found <a href="https://github.com/ma489/pyspark-ml-classification">here</a>.</p>

<h2 id="exploratory-data-analysis">Exploratory Data Analysis</h2>

<p>First, an initial exploratory data analyis is conducted. The full dataset contains 48,842 instances, but 45,222 when instances with unknown values are removed (this latter subset is used in this analysis). The dataset comprises 15 variables - a mixture of 14 continuous and categorical demographic features (used in this analysis as explanatory variables) and 1 categorical variable (<code class="language-plaintext highlighter-rouge">class</code>) indicating whether this person earns ≤ $50<em>k</em> or &gt; $50<em>k</em> (used in this analysis as the dependent variable). Table 1 lists the variables, their data types, and examples.</p>

<p><img src="/assets/img/pyspark_ml_table1.png" alt="Table1" class="center-image" /></p>

<p>Note that the variable <code class="language-plaintext highlighter-rouge">fnlwgt</code> is (essentially) an estimate, by the Census, of how many people share these characteristics. This variable is dropped in this analysis, as it is just metadata (correlation calculations confirm its irrelevance).</p>

<p>Table 2 provides selected summary statistics per-Class: the mean for continuous variables, and the mode for categorical variables. We can see that on average, those earning above $50k are (1) older (44 years vs 34.7 years), (2) have more years of education (11.6 vs 9.6) and (3) work more hours per week (45.7 vs 39.4). All of which make intuitive sense.</p>

<p><img src="/assets/img/pyspark_ml_table2.png" alt="Table2" class="center-image" /></p>

<p>Figure 1 below confirms the correlations suggested by the per-Class means of continuous variables above: the top 3 variables positively correlated with the over $50k class are age, education-num, and hours-per-week. Figure 2 further examines selected <em>categorical</em> variables by <em>Class</em>.</p>

<p><img src="/assets/img/pyspark_ml_fig1-2.png" alt="Table2" class="center-image" /></p>

<h2 id="modelling">Modelling</h2>

<p>The research question of interest that this report considers is, concretely: <em>Given US Census demographic data, how accurately can the individuals in the dataset be classified as earning either above or below $50k?</em>. This question is approached as a Classification problem, and modelling was performed using PySpark and its ML package <a href="#references">[2]</a>.</p>

<p>The data obtained was already split into training and test sets. Both training and test data sets were preprocessed, for example to remove missing values from all columns, and to apply consistent formatting to the Class column in order to then convert it to the binary 0/1 valed column. All feature columns except <code class="language-plaintext highlighter-rouge">fnlwgt</code> were kept. The data was then transformed using a <code class="language-plaintext highlighter-rouge">FeatureHasher</code> and <code class="language-plaintext highlighter-rouge">VectorAssembler</code> from PySpark’s MLlib package, to combine the continous and categorical variables. Then a <code class="language-plaintext highlighter-rouge">MaxAbsScaler</code>, from PySpark’s MLlib package, was applied to the data. This series of data transformation steps were combined using a Pipeline in order to easily define and reproduce the steps.</p>

<p>Finally, different Classifier models were trained on the training data set, and evaluated on a the test set. The AUROC scores for the different Classifiers is given in Table 3 - Logistic Regression was found to perform best, with an AUROC of 0.9022, and Naive Bayes the worst at 0.363.</p>

<p><img src="/assets/img/pyspark_ml_table3.png" alt="Table3" class="center-image" /></p>

<h2 id="interpretation--conclusion">Interpretation &amp; Conclusion</h2>

<p>These results show that, with correctly processed training data, and the right selection of model (Logistic Regression), it is possible to accurately classify individuals as earning either above or below $50k with an AUROC of 0.9022. Unfortunately, the application of a <code class="language-plaintext highlighter-rouge">VectorAssembler</code> during pre-processing makes it difficult to extract feature importance information in detail, however the most important features could be expected to align with the positively correlated features identified in the exploratory data analysis.</p>

<hr />

<h2 id="references">References</h2>

<ol>
  <li>Becker, B and Kohavi, R. (1996). <a href="https://doi.org/10.24432/C5XW20">Census Income dataset</a>. UCI Machine Learning Repository.</li>
  <li><a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html">PySpark ML package</a>.</li>
</ol>]]></content><author><name>Mansour Ahmed</name><email>mail@mansourahmed.com</email></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Exploring Language in Classic Fiction with Data and R</title><link href="http://localhost:4000/2023/04/analysing-language-fiction/" rel="alternate" type="text/html" title="Exploring Language in Classic Fiction with Data and R" /><published>2023-04-19T08:01:00-04:00</published><updated>2023-04-19T08:01:00-04:00</updated><id>http://localhost:4000/2023/04/analysing-language-fiction</id><content type="html" xml:base="http://localhost:4000/2023/04/analysing-language-fiction/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In this post, I will present an exploratory data analysis and visualisation of natural language data from a collection of 26 fiction books by six English-language authors. The data was downloaded from the <a href="https://www.gutenberg.org/">Project Gutenberg</a> website <a href="#references">[1]</a>. For the purposes of this analysis, we treat each book as a document, and the collection of 26 books as a corpus. The text content of each book was pre-processed to remove irrelevant text added by Project Gutenberg.</p>

<p>The analysis is performed using <a href="https://www.r-project.org/">R</a>. As part of the analysis, I applied dimensionality reduction techniques such as <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a> (PCA), <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling">Multidimensional Scaling</a> (MDS), and <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>. I also used clustering methods such as <a href="https://en.wikipedia.org/wiki/K-means_clustering">K-means</a> and <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">Hierarchical clustering</a>.</p>

<h2 id="initial-exploratory-data-analysis">Initial Exploratory Data Analysis</h2>

<p>For an initial exploratory analysis of the data, a subset of the corpus containing 2 documents was selected (hence, the ’Corpus Subset’). The Corpus Subset comprises: <em>Great Expectations</em>, by Charles Dickens, and <em>The Sign of the Four</em>, by Sir Arthur Conan Doyle. Statistics of the Corpus Subset texts are presented in Table 1.</p>

<p><img src="/assets/img/analysing_language_table1.jpg" alt="Table1" class="center-image" /></p>

<p>The statistics in Table 1 give some insight into the writing style of the documents. For example, we see that Dickens’ sentences were longer than Doyle’s, containing 19.2% more words on average. Dickens’ book is also longer, containing 4 times as many words as Doyle’s. This difference in word count and sentence length possibly reflects the nature and era of the texts and their intended audiences: Dickens’ text is a relatively older novel that depicts and critiques the social and cultural landscape of 19th Century England (requiring more words and more complex sentences) whereas Doyle’s work is of the entertainment genre (detective mystery).</p>

<p>Figure 1 presents the top ten words in each Corpus Subset document based on tf-idf (the definition of tf-idf used is the one implemented by bind tf idf in tidytext <a href="#references">[2]</a>). Pre-processing was performed to remove the ‘stop’ words (based on a list of stop words in the <code class="language-plaintext highlighter-rouge">tidytext</code> package <a href="#references">[2]</a>). From Figure 1, we can quickly see the key characters and themes of each book. For example, ‘Holmes’ is the name of the main character in ‘The Sign of Four’, and ‘Joe’ is the name of one of the main characters in ‘Great Expectations’.</p>

<p><img src="/assets/img/analysing_language_fig1.png" alt="Figure1" class="center-image" /></p>

<p>Similarly, Figure 2 shows the most common words in each of the subset documents, in the form of word clouds, and gives insight into the key characters in the documents.</p>

<p><img src="/assets/img/analysing_language_fig2.png" alt="Figure2" class="center-image" /></p>

<p>Figure 3 provides another view into the language used in this Corpus Subset: the bigraph shows common bigrams (based on frequency) and their interconnections. There is a large cluster at the top, centered on the words ‘miss’ and ‘dear’; possibly reflecting the formal language used in the books, both of which were published in the 19th century.</p>

<p><img src="/assets/img/analysing_language_fig3.png" alt="Figure3" class="center-image" /></p>

<h2 id="similarity-of-language">Similarity of Language</h2>

<p>We now turn our attention to the full data set containing the corpus of all 26 books, which is analysed to assess the similarity of language used across the corpus. The data was first pre-processed in the following way:</p>

<ol>
  <li>Each document in the corpus is tokenized into bigrams</li>
  <li>The bigrams are filtered to remove any bigrams that contain at least one stop word, producing p bigrams. In this corpus, p is ~145,000.</li>
  <li>The <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"><em>tf-idf</em></a> of the remaining bigrams is calculated (using <code class="language-plaintext highlighter-rouge">bind_tf_idf</code> from <code class="language-plaintext highlighter-rouge">tidytext</code> <a href="#references">[2]</a>)
    <ul>
      <li>tf-idf is used here rather than raw term frequency count because we want to account for varying document lengths</li>
    </ul>
  </li>
  <li>A data matrix (<strong>DataMatrix</strong>) is constructed, converting the data from wide format to long format
    <ul>
      <li>This matrix has dimensions 26 × p. That is, 26 rows (one for each document), and p columns (one for each bigram).</li>
      <li>Each cell in the matrix contains the corresponding tf-idf value. The value in <strong>DataMatrix<sub>i,j</sub></strong> is the tf-idf of bigram j in document i.</li>
    </ul>
  </li>
</ol>

<h3 id="distance-as-similarity">Distance as Similarity</h3>

<p>A distance matrix is then calculated from the <strong>DataMatrix</strong>, and visualised in Figure 4; it shows that document 13 in the corpus (<em>The Eyes Have It</em>, by Dick) is most distance to the rest, followed by document 23 (<em>The Missing Will</em> by Christie). In this analysis, we consider distance in the tf-idf vector space as a signal of similarity of language - i.e. documents whose tf-idf vectors (represented in <strong>DataMatrix</strong>) that are close in that space use similar language.</p>

<p><img src="/assets/img/analysing_language_fig4.png" alt="Figure4" class="center-image" /></p>

<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>

<p>Next, Dimensionality Reduction is performed on the DataM atrix. Reduced dimensions are necessary for reducing the computational cost of the subsequent clustering operations, as well as facilitating visualisation. Dimensionality Reduction (down to two dimensions) is performed using three techniques (1) <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a> (PCA), (2) <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling">Multidimensional Scaling</a> (MDS), and (3) PCA followed by <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>; the results of which are visualised in Figures 5, 6, and 7 respectively. Importantly, these methods preserve ’closeness’ (points that are close in the higher-dimensional space are close in the lower-dimensional space).</p>

<p>Notes on the implementation: In method (1), the first two principal components were found to explain only 18% of the variance in the data. In method (3), PCA is performed first as t-SNE is computationally expensive, and using it directly is impractical on a dataset of this size. Finally, the two outlier points mentioned earlier - documents 13 and 23 - were removed to improve the visualisations by ’zooming in’ on the main cluster points.</p>

<p><img src="/assets/img/analysing_language_fig5.png" alt="Figure5" class="center-image" /></p>

<p><img src="/assets/img/analysing_language_fig6.png" alt="Figure6" class="center-image" /></p>

<p><img src="/assets/img/analysing_language_fig7.png" alt="Figure7" class="center-image" /></p>

<h3 id="clustering">Clustering</h3>

<p>In order to group the documents by similarity of language, <em>k-means</em> clustering was performed on the three reduced-dimension data sets. Figures 8, 9, and 10 show the resultant clusters (book titles are replaced by the authors’ initials in Figures 8 and 9, for better visualisation). In each case, the values for <em>K</em> were selected using a combnination the <a href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)">elbow method</a> (using <code class="language-plaintext highlighter-rouge">fviz_nbclust</code> from the <code class="language-plaintext highlighter-rouge">factoextra</code> R package) and visual inspection of the resulting clusters. We can see that the third approach in Figure 11 (PCA followed by t-SNE followed by K-Means) results in the clearest separation of clusters; we see some of the works by the same author are grouped to together, e.g. three of Dickens’ works appear together in the middle red cluster, and two of Austen’s works appear together in the pink cluster (bottom left).</p>

<p><img src="/assets/img/analysing_language_fig8.png" alt="Figure8" class="center-image" /></p>

<p><img src="/assets/img/analysing_language_fig9.png" alt="Figure9" class="center-image" /></p>

<p><img src="/assets/img/analysing_language_fig10.png" alt="Figure10" class="center-image" /></p>

<p>Hierarchical Clustering was also performed on the documents, in the high-dimensional space. Figure 12 shows the resultant dendrogram. Towards the left of the diagram, we see a cluster for The Eyes Have It (document 13, the outlier, from earlier), suggesting this book’s language is quite different from the other books. Towards the right we see clusters for Doyle’s books, The Sign of the Four and A Study in Scarlet.</p>

<p><img src="/assets/img/analysing_language_fig11.png" alt="Figure11" class="center-image" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>In conclusion, this report has demonstrated that a number of techniques can be applied to analyse and visualise unstructured textual data and identify patterns. While <em>some</em> patterns do emerge when clustering (i.e. subsets of the same author’s works appearing together), in all methods there are limitations, as shown by the lack of intuitively-understood clusters.</p>

<p>Prior to performing clustering, this report’s author had some expectations of the number of clusters that would possibly emerge in this corpus: perhaps six clusters reflecting the six authors, or three clusters reflecting the three genres that might broadly categorise the corpus (<em>Detective Mystery</em> - Christie and Doyle, <em>Sci-Fi</em> - Wells and Dick, <em>Victorian Life</em> - Dickens and Austen), or clusters around publication dates (intuitively: language changes over time). Further analysis and exploration should pursue uncovering these deeper patterns.</p>

<hr />

<h2 id="references">References</h2>

<ol>
  <li>Project Gutenberg. (n.d.). Retrieved April 2023, from <a href="https://www.gutenberg.org/">www.gutenberg.org</a>.</li>
  <li>Silge, J., &amp; Robinson, D. (2016). <a href="https://joss.theoj.org/papers/10.21105/joss.00037">Tidytext: Text mining and analysis using tidy data principles</a>. <em>R. Journal of Open Source Software, 1(3), 37.</em></li>
</ol>

<hr />

<p>Note: This post is based on a report written for an assignment as part of my degree in <em>Machine Learning &amp; Data Science</em> at <a href="https://www.imperial.ac.uk">Imperial College London</a>. The assignment was part of <em>Exploratory Data Analysis &amp; Visualisation</em> module taught by the inspiring, knowledgable and supportive Dr. J Martin (all credit due for the assignment setting, the goals and objectives of the analysis, and the selection and provision of the dataset).</p>]]></content><author><name>Mansour Ahmed</name><email>mail@mansourahmed.com</email></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Evaluating the Robustness of Linear Discriminant Analysis</title><link href="http://localhost:4000/2023/02/lda-evaluation/" rel="alternate" type="text/html" title="Evaluating the Robustness of Linear Discriminant Analysis" /><published>2023-02-01T07:02:00-05:00</published><updated>2023-02-01T07:02:00-05:00</updated><id>http://localhost:4000/2023/02/lda-evaluation</id><content type="html" xml:base="http://localhost:4000/2023/02/lda-evaluation/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In this blog post, I will evaluate the performance of <a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">Linear Discriminant Analysis</a> (<strong>LDA</strong>) in a classification task, when LDA’s underlying assumptions are violated. Through a series of simulations, the effects of different types of misspecifications, and different severities of those misspecifications, on LDA’s performance are explored <a href="#references">[1]</a>.</p>

<h2 id="linear-discriminant-analysis">Linear Discriminant Analysis</h2>

<p>As a reminder, LDA is a statistical method that can be used as a supervised classification model.
LDA makes the following assumptions <a href="#references">[2]</a> (however, it is believed that LDA is ”robust to slight violations of these” <a href="#references">[2]</a>):</p>

<ol>
  <li>The independent variables have (Multivariate) Normal distribution within each class</li>
  <li>The Variance/covariance of the predictors is equal across all classes, whereas the mean (vector) is class-specific</li>
  <li>Observations in the sample are independent</li>
  <li>Predictive power can decrease in the presence of multicollinearity</li>
</ol>

<h2 id="the-classification-task">The Classification Task</h2>

<p>The task is that of binary classification of observations in a dataset. The dataset (<em>n</em> = 200) is composed of two covariates, X1 and X2, simulated from a multivariate Normal distribution, and a binary response variable Y representing two classes (0 and 1). The mean vector of the covariates is class-specific (i.e. dependent on the value of the <em>Y</em>), but the variance-covariance matrix is the same across classes. The initial dataset is equally balanced between the two classes (100 observations of each). To illustrate, a subset of an example dataset is given in Table 1. A complete example dataset is visualised in Figure 1.</p>

<p><img src="/assets/img/lda_tab1_fig1.png" alt="Fig1" class="center-image" /></p>

<h2 id="experiments">Experiments</h2>

<h3 id="methodology">Methodology</h3>

<p>Classification performance here is measured in terms of misclassification rate:</p>

\[\frac{\text{Number of Misclassified Observations}}{\text{Total Number of Observations}}\]

<p>The three misspecification types (MSTs) that are explored are as follows (misspecifications are
in the form of input datasets that violate LDA assumptions):</p>

<ol>
  <li><strong>MST1</strong>: “The covariate data come from a distribution with a heavier tail than the normal, e.g. a t-distribution” <a href="#references">[1]</a>.</li>
  <li><strong>MST2</strong>: “The sampling of classes is imbalanced. This means that the proportions of observations in the two classes in the training sample are not representative of the proportions in the population” <a href="#references">[1]</a>.</li>
  <li><strong>MST3</strong>: “The data are ‘poisoned’. This means that the class labels of a small proportion of
the of the training observations are switched” <a href="#references">[1]</a>.</li>
</ol>

<p>To conduct this exploration, simulations were performed for each of the 3 misspecification types
as follows (for brevity, the <code class="language-plaintext highlighter-rouge">R</code> code used is not included here).</p>

<p>As a first step, 1000 datasets were generated, in 10 batches of 100, where each successive batch
had an increasing severity of the misspecification being explored.</p>

<p>Then, for each batch of 100 datasets, the following steps are performed:</p>
<ol>
  <li>For each of the 100 datasets of 200 observations in the batch:
    <ul>
      <li>Split the dataset such that 70% of observations are used for training and 30% are used for testing.</li>
      <li>Fit an LDA model, using <code class="language-plaintext highlighter-rouge">MASS</code> <a href="#references">[3]</a>, on the training subset and classify the test subset</li>
      <li>Calculate the misclassification rate over the test subset</li>
    </ul>
  </li>
  <li>Calculate the mean misclassification rate (MMR) over the batch</li>
  <li>Plot, using <code class="language-plaintext highlighter-rouge">ggplot2</code> <a href="#references">[4]</a>, the MMR against a measure of misspecification severity for that batch</li>
</ol>

<p>As a baseline for comparison: for the initial dataset without deliberate misspecification, the MMR over 1000 simulated datasets was <strong>12.49%</strong>.</p>

<h3 id="mst1">MST1</h3>

<p>In this simulation, the <code class="language-plaintext highlighter-rouge">stats</code> package in <code class="language-plaintext highlighter-rouge">R</code> <a href="#references">[5]</a> was used to generate datasets that followed the <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution"><em>t</em>-distribution</a>, which has heavier tails than the <a href="https://en.wikipedia.org/wiki/Normal_distribution">Normal distribution</a> (i.e. positive excess <a href="https://en.wikipedia.org/wiki/Kurtosis">kurtosis</a>). The <a href="https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)">degrees of freedom</a> control the kurtosis of the t-distribution: by decreasing the degrees of freedom for each dataset batch, the positive excess kurtosis is increased, and thus the misspecification severity is increased. Figure 2 shows a weak negative <a href="https://en.wikipedia.org/wiki/Correlation">correlation</a> between degrees of freedom and MMR. As the misspecification severity increases, MMR increases from <strong>12.1%</strong> to <strong>14.3%</strong>.</p>

<p><img src="/assets/img/lda_fig2.png" alt="Fig2" class="center-image" /></p>

<h3 id="mst2">MST2</h3>

<p>In this simulation, to increase the severity of the misspecification, the proportion of Class 1 observations in the training sample was reduced in each batch, from 0.5 (i.e. the classes were balanced in representation in training) down to 0.025 (classes are imbalanced: fewer observations of Class 1). Figure 3 shows a strong negative correlation between class balance and MMR. As the misspecification severity (class imbalance) increases, MMR increases from <strong>12%</strong> to <strong>23%</strong>.</p>

<p><img src="/assets/img/lda_fig3.png" alt="Fig3" class="center-image" /></p>

<h3 id="mst3">MST3</h3>

<p>In this simulation, to increase the severity of the misspecification, the proportion of observations with incorrect (switched) labels in the training sample was increased in each batch, from 0 (i.e. no poison: all observations are correctly labelled) up to 0.95. Figure 3 shows a strong positive correlation between poisoned-proportion and MMR. As the misspecification severity (poisoned-proportion) increases, MMR increases from <strong>12.3%</strong> to <strong>82.8%</strong>.</p>

<p><img src="/assets/img/lda_fig4.png" alt="Fig4" class="center-image" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>We have seen how LDA performance changes under different types of, and severities of, misspecifications. While LDA is relatively robust under violations of its Normal distribution assumption (<strong>MST1</strong>), its performance drops more rapidly under more egregious violations such as <strong>MST2</strong> and <strong>MST3</strong>, where it is given misrepresentative and incorrect training data respectively.</p>

<hr />

<h2 id="references">References</h2>

<ol>
  <li>Hallsworth, C. <em>MLDS Supervised Learning</em>. Imperial College London. 2022.</li>
  <li>Wikipedia: <a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">Linear Discriminant Analysis</a>.</li>
  <li>Venables WN, Ripley BD (2002). <a href="https://www.stats.ox.ac.uk/pub/MASS4/">Modern Applied Statistics with S</a>, Fourth edition. Springer, New York. ISBN 0-387-95457-0,</li>
  <li>Wickham, H. <a href="https://ggplot2.tidyverse.org/">ggplot2</a>: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.</li>
  <li>R Core Team (2022). <a href="https://www.R-project.org">R: A language and environment for statistical computing</a>. R Foundation for Statistical Computing, Vienna, Austria.</li>
</ol>

<hr />

<p>Note: This post is based on a report written for an assignment as part of my degree in <em>Machine Learning &amp; Data Science</em> at <a href="https://www.imperial.ac.uk">Imperial College London</a>. The assignment was part of <em>Supervised Learning</em> module taught by Dr. C Hallsworth (all credit due for the assignment setting and the goals and objectives of the analysis).</p>]]></content><author><name>Mansour Ahmed</name><email>mail@mansourahmed.com</email></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">2022 Reading List</title><link href="http://localhost:4000/2022/12/2022-reading-list/" rel="alternate" type="text/html" title="2022 Reading List" /><published>2022-12-11T07:02:00-05:00</published><updated>2022-12-11T07:02:00-05:00</updated><id>http://localhost:4000/2022/12/2022-reading-list</id><content type="html" xml:base="http://localhost:4000/2022/12/2022-reading-list/"><![CDATA[<p>A much shorter list of <em>books</em> this year, as I’m reading more academic papers instead</p>

<h3 id="books">Books</h3>

<ul>
  <li><a href="https://www.goodreads.com/book/show/46074.Limbo">Limbo</a> (Lubrano, 2003)</li>
  <li><a href="https://www.goodreads.com/en/book/show/44244975-the-ethical-algorithm">The Ethical Algorithm</a> (Kearns &amp; Roth, 2019)</li>
</ul>

<h3 id="audiobooks">Audiobooks</h3>

<ul>
  <li><a href="https://www.goodreads.com/en/book/show/60149538-virtual-society">Virtual Society</a> (Narula, 2022)</li>
  <li><a href="https://www.goodreads.com/en/book/show/49933627-black-spartacus">Black Spartacus</a> (Hazareesingh, 2020)</li>
  <li><a href="https://www.goodreads.com/book/show/30354426-solve-for-happy">Solve For Happy</a> (Gawdat, 2017)</li>
  <li><a href="https://www.goodreads.com/book/show/1835405.When_Prophecy_Fails">When Prophecy Fails</a> (Festinger et al, 1956)</li>
  <li><a href="https://www.goodreads.com/book/show/45748989-indistractable">Indistractable</a> (Eyal, 2019)</li>
  <li><a href="https://www.goodreads.com/book/show/97641.Nice_Girls_Don_t_Get_the_Corner_Office">Nice Girls Don’t Get the Corner Office</a> (Frankel, 2010)</li>
</ul>

<h3 id="papers">Papers</h3>

<ul>
  <li>Floridi, L., &amp; Cowls, J. (2019). <a href="https://doi.org/10.1162/99608f92.8cd550d1">A Unified Framework of Five Principles for AI in Society</a>. <em>Harvard Data Science Review, 1(1)</em>.</li>
  <li>Narayanan, A. &amp; Shmatikov, V. (2008). <a href="https://doi.org/10.1109/SP.2008.33">Robust De-anonymization of Large Sparse Datasets</a>. <em>2008 IEEE Symposium on Security and Privacy (sp 2008). 111–125</em>.</li>
  <li>Buolamwini, J., &amp; Gebru, T. (2018). <a href="https://proceedings.mlr.press/v81/buolamwini18a.html">Gender shades: Intersectional accuracy disparities in commercial gender classification</a>. <em>PMLR Conference on fairness, accountability and transparency (pp. 77-91)</em>.</li>
  <li>Awad, E., Dsouza, S., Kim, R. et al. <a href="https://doi.org/10.1038/s41586-018-0637-6">The Moral Machine experiment</a>. <em>Nature 563, 59–64 (2018)</em>.</li>
  <li>Ribeiro, M. T., Singh, S. &amp; Guestrin, C. (2016). <a href="https://doi.org/10.1145/2939672.2939778">“Why Should I Trust You?”: Explaining the Predictions of Any Classifier</a>. <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 1135-1144</em>.</li>
  <li>Ameisen, E. (2020). <em>(Chapter 11)</em>. In <a href="https://www.oreilly.com/library/view/building-machine-learning/9781492045106/"><em>Building machine learning powered applications: going from idea to product</em></a>. First edition. O’Reilly Media, Inc.</li>
</ul>]]></content><author><name>Mansour Ahmed</name><email>mail@mansourahmed.com</email></author><summary type="html"><![CDATA[A much shorter list of books this year, as I’m reading more academic papers instead]]></summary></entry></feed>